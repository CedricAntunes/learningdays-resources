---
title: "Pruebas de hipótesis: Resumiendo información acerca de los efectos causales"
author: "Fill In Your Name"
date: "`r format(Sys.time(), '%d %B %Y')`"
bibliography: ../learningdays-book.bib
biblio-style: apalike
link-citations: yes
colorlinks: yes
fig_caption: yes
header-includes: |
   \setbeamertemplate{footline}{\begin{beamercolorbox}{section in head/foot}
   \includegraphics[height=.5cm]{../Images/egap-logo.png} \hfill
   \insertframenumber/\inserttotalframenumber \end{beamercolorbox}}
   \usepackage{tikz}
   \usepackage{tikz-cd}
   \usepackage{textpos}
   \usepackage{booktabs,multirow,makecell}
output:
  beamer_presentation: 
    slide_level: 2
    keep_tex: yes
    toc: yes
    pandoc_args: --toc
    fig_caption: yes
  revealjs::revealjs_presentation:
    fig_caption: true
    theme: default
    highlight: pygments
    center: false
    transition: fade
    smart: false
    self_contained: false
    reveal_plugins: ["notes", "search", "chalkboard"]
    pandoc_args: [ "--toc" ]
    reveal_options:
      slideNumber: true
      previewLinks: true
      chalkboard:
        theme: whiteboard
        toggleNotesButton: false
---

```{r setup, include=FALSE}
source("rmd_setup.R")
# Load all the libraries we need
library(here)
library(tidyverse)
library(kableExtra)
library(DeclareDesign)
library(estimatr)
library(styler)
library(coin)
library(multcomp)
library(devtools)
library(randomizr)
library(rcompanion) ## for pairwisePermutationTest()
```



#  El papel de las pruebas de hipótesis en la inferencia causal {.allowframebreaks}

## Puntos clave para esta lección  {.allowframebreaks}

- La inferencia estadística (por ejemplo, las pruebas de hipótesis y los intervalos de
confianza) requiere **inferencia**, es decir, razonamiento sobre lo no observado.

- Los valores $p$ requieren distribuciones de probabilidad.

- Aleatorización (o diseño) + una hipótesis + una función de estadística de prueba $\rightarrow$ distribuciones de probabilidad que representan la hipótesis (distribuciones de referencia)

- Valores observados de las estadísticas de prueba + Distribución de referencia $\rightarrow$ valor $p$.

## El papel de las pruebas de hipótesis en la inferencia causal  {.allowframebreaks}

- El **problema fundamental de la inferencia causal** es que sólo podemos ver una variable de resultado potencial para cualquier unidad.

- Por lo tanto, si para Jake se produce un efecto causal contrafactual del tratamiento $T$, cuando $y_{text{Jake},T=1} \ne y_{text{Jake},T=0}$, entonces ¿cómo podemos aprender sobre el efecto causal?

- Una solución es la **[estimación](estimation-slides.Rmd) de los promedios de los efectos causales** (el ATE, ITT, LATE).

- Esto es lo que llamamos el enfoque de Neyman.


- Otra posible solución es hacer **afirmaciones** o **suposiciones** sobre los efectos causales.

- Podríamos decir: "Creo que el efecto sobre Jake es 5" o "Este experimento no ha tenido ningún efecto sobre nadie". Y entonces podríamos preguntarnos "¿Cuánta evidencia tiene este experimento sobre esa afirmación?"

- Esta evidencia se resume en un valor $p$.

- A esto lo llamamos enfoque de Fisher.

- El enfoque de las pruebas de hipótesis para la inferencia causal no nos brinda una suposición, sino que nos dice *cuánta evidencia o información obtenemos del diseño de la investigación sobre una afirmación causal*.

- La estimación nos permite hacer mejores suposiciones, pero no nos dice qué tanto sabemos sobre esas suposiciones.
- Por ejemplo, una  suposición con $N=10$ parece decirnos menos sobre el efecto que $N=1000$.
 - Por ejemplo, una suposición cuando el 95% de $Y=1$ y el 5% de $Y=0$ parece decirnos menos que cuando las variables de resultado se dividen por igual entre 0 y 1.
- Casi siempre reportamos ambas, ya que esto nos ayuda a tomar decisiones: "Nuestra mejor suposición del efecto del tratamiento fue 5, y pudimos rechazar la idea de que el efecto fuera 0 ($p$=.01)".


# Temas básicos de las prueba de hipótesis {.allowframebreaks}

## Componentes de una prueba de hipótesis {.allowframebreaks}

 - Una **hipótesis** es una afirmación sobre una relación entre variables de resultado potenciales.

 - Una **estadística de prueba** resume la relación entre el tratamiento y las variables de resultado observadas.

 - El **diseño** nos permite vincular la hipótesis y la estadística de prueba: podemos calcular una estadística de prueba que describa una relación entre variables de resultado potenciales.

 - El **diseño** también nos indica cómo generar una *distribución* de las posibles estadísticas de prueba sugeridas por la hipótesis.
 
 - Un valor **$p$** describe la relación entre nuestra estadística de prueba observada y la distribución de las posibles estadísticas de prueba hipotéticas.


```{r echo=FALSE}
## Primero, crear algunos datos,
##  y0 es la variable de resultado potencial para control
N <- 10
y0 <- c(0, 0, 0, 1, 1, 3, 4, 5, 190, 200)
## Diferentes efectos del tratamiento a nivel individual
tau <- c(10, 30, 200, 90, 10, 20, 30, 40, 90, 20)
## y1 es la variable de resultado potencial para tratamiento
y1 <- y0 + tau
# sd(y0)
# mean(y1)-mean(y0)
# mean(tau)
## T es el tratamiento asignado
set.seed(12345)
T <- complete_ra(N)
## Y es la variable de resultado observada
Y <- T * y1 + (1 - T) * y0
## Los datos
dat <- data.frame(Y = Y, T = T, y0 = y0, tau = tau, y1 = y1)
dat$Ybin <- as.numeric(dat$Y > 100)
# dat
# pvalue(oneway_test(Y~factor(T),data=dat,distribution=exact(),alternative="less"))
# pvalue(wilcox_test(Y~factor(T),data=dat,distribution=exact(),alternative="less"))
```


```{r echo=FALSE}
## Crear un grupo de datos más grande
##  y0 es la variable de resultado potencial para control
bigN <- 60
set.seed(12345)
bigdat <- data.frame(y0 = c(rep(0, 20), rnorm(20, mean = 3, sd = .5), rnorm(20, mean = 150, sd = 10)))
## Diferentes efectos del tratamiento a nivel individual
bigdat$tau <- c(rnorm(20, mean = 10, sd = 2), rnorm(20, mean = 20, sd = 5), rnorm(20, mean = 5, sd = 10))
## y1 es la variable de resultado potencial para tratamiento
bigdat$y1 <- bigdat$y0 + bigdat$tau
# sd(y0)
# mean(y1)-mean(y0)
# mean(tau)
## T es el tratamiento asignado
set.seed(12345)
bigdat$T <- complete_ra(bigN)
## Y es la variable de resultado observada
bigdat$Y <- with(bigdat, T * y1 + (1 - T) * y0)
## Los datos
bigdat$Ybin <- as.numeric(bigdat$Y > quantile(bigdat$Y, .85))
```

## Una hipótesis es una afirmación o modelo de una relación entre posibles variables de resultado {.allowframebreaks}

```{r}
kableExtra::kable(dat, col.names = c("Outcome", "Treatment", "$y_{i,0}$", "ITE", "$y_{i,1}$", "$Y>0$"), escape = FALSE)
```

Por ejemplo, la hipótesis nula de ausencia de efectos, débil o tajante, es $H_0:y_{i,1} = y_{i,0}$


## Las estadísticas de pruebas resumen las relaciones entre el tratamiento y las variables de resultado  {.allowframebreaks}

```{r, echo=TRUE}
## La estadística de prueba de diferencia de medias
meanTT <- function(ys, z) {
  mean(ys[z == 1]) - mean(ys[z == 0])
}
## Rangos de la estadística de prueba de diferencia de medias
meanrankTT <- function(ys, z) {
  ranky <- rank(ys)
  mean(ranky[z == 1]) - mean(ranky[z == 0])
}

observedMeanTT <- meanTT(ys = Y, z = T)
observedMeanRankTT <- meanrankTT(ys = Y, z = T)
observedMeanTT
observedMeanRankTT
```

## El diseño conecta la estadística de prueba y la hipótesis {.allowframebreaks}

Lo que observamos para cada persona $i$ ($Y_i$) es lo que habríamos observado en el tratamiento ($y_{i,1}$) **o** lo que habríamos observado en el control ($y_{i,0}$).

$$Y_i = T_i y_{i,1} + (1-T_i)* y_{i,0}$$

Entonces, si $y_{i,1}=y_{i,0}$ por lo tanto $Y_i = y_{i,0}$.

Lo que *observamos realmente* es lo que *habríamos observado en la condición de control*.

## El diseño guía la creación de una distribución de estadísticas de prueba hipotéticas {.allowframebreaks}

Necesitamos saber cómo repetir nuestro experimento:

```{r, echo=TRUE}
repeatExperiment <- function(N) {
  complete_ra(N)
}
```

Luego lo repetimos, calculando la estadística de prueba implícita por la hipótesis y el diseño cada iteración:

```{r reps, echo=TRUE, cache=TRUE}
set.seed(123456)
possibleMeanDiffsH0 <- replicate(
  10000,
  meanTT(ys = Y, z = repeatExperiment(N = 10))
)
set.seed(123456)
possibleMeanRankDiffsH0 <- replicate(
  10000,
  meanrankTT(ys = Y, z = repeatExperiment(N = 10))
)
```

## Planear las distribuciones de aleatoriedad bajo la hipótesis nula {.allowframebreaks}

```{r fig.cap=" Un ejemplo de uso del diseño del experimento para probar una hipótesis con dos estadísticas de prueba diferentes.", results='asis', echo=FALSE, fig.align='center'}
par(mfrow = c(1, 2), mgp = c(2, .5, 0), mar = c(3, 3, 0, 0), oma = c(0, 0, 3, 0))
plot(density(possibleMeanDiffsH0),
  ylim = c(0, .04),
  xlim = range(possibleMeanDiffsH0),
  lwd = 2,
  main = "", # Estadística de prueba de diferencia de medias ",
  xlab = "Diferencia de medias consistentes con H0",
  cex.lab = 1.0, cex.axis = 1
)
rug(possibleMeanDiffsH0)
rug(observedMeanTT, lwd = 3, ticksize = .51)
text(observedMeanTT + 20, .022, "Estadística de prueba observada")

plot(density(possibleMeanRankDiffsH0),
  lwd = 2,
  ylim = c(0, .45),
  xlim = c(-10, 10), # range(possibleMeanDiffsH0),
  main = "", # Rangos de la estadística de prueba de diferencia de medias",
  xlab = " Diferencia de medias de rangos consistentes con H0",
  cex.lab = 1.0, cex.axis = 1
)
rug(possibleMeanRankDiffsH0)
rug(observedMeanRankTT, lwd = 3, ticksize = .9)
text(observedMeanRankTT, .45, " Estadística de prueba observada")

mtext(
  side = 3, outer = TRUE, cex = 1.5,
  text = expression(paste("Distribuciones de las estadísticas de prueba consistentes con el diseño y ", H0:y[i1] == y[i0]))
)
```

## Los valores $p$ resumen los planes

¿Cómo debemos interpretar los valores $p$? (Nótese que son de una cola)

```{r calcpvalues, echo=TRUE}
pMeanTT <- mean(possibleMeanDiffsH0 >= observedMeanTT)
pMeanRankTT <- mean(possibleMeanRankDiffsH0 >= observedMeanRankTT)
pMeanTT
pMeanRankTT
```

## Cómo hacer esto en R: COIN

```{r coinexample, echo=TRUE}
## usando el paquete coin
library(coin)
set.seed(12345)
pMean2 <- coin::pvalue(oneway_test(Y ~ factor(T),
  data = dat,
  distribution = approximate(nresample = 1000), alternative = "less"
))
dat$rankY <- rank(dat$Y)
pMeanRank2 <- coin::pvalue(oneway_test(rankY ~ factor(T),
  data = dat,
  distribution = approximate(nresample = 1000), alternative = "less"
))
pMean2
pMeanRank2
```

## Cómo hacer esto en R: RItools  {.allowframebreaks}

 Primero instalar la versión en desarrollo del paquete de RItools 

```{r installritools, eval=FALSE, echo=TRUE, results='hide',warnings=FALSE,cache=FALSE}
# dev_mode() ## no instalar el paquete de manera global
renv::install("markmfredrickson/RItools@randomization-distribution",
  force = TRUE
)
# dev_mode()
```

Luego use la función `RItest`.

```{r useritools, eval=FALSE,echo=TRUE,cache=FALSE}
# dev_mode()
library(RItools)
thedesignA <- simpleRandomSampler(total = N, z = dat$T, b = rep(1, N))
pMean4 <- RItest(
  y = dat$Y, z = dat$T, samples = 1000, test.stat = meanTT,
  
  sampler = thedesignA
)
pMeanRank4 <- RItest(
  y = dat$Y, z = dat$T, samples = 1000, test.stat = meanrankTT,
  
  sampler = thedesignA
)
pMean4
pMeanRank4
# dev_mode() ## and turn off dev_mode
```

```{r ritoolsoutput, echo=TRUE, eval=FALSE, tidy=FALSE}
pMean4
Call:  RItest(y = dat$Y, z = dat$T, test.stat = meanTT, sampler = thedesignA,
          samples = 1000)

                        Value Pr(>x)
Estadística de Prueba Observada -49.6   0.78

pMeanRank4
Call:  RItest(y = dat$Y, z = dat$T, test.stat = meanrankTT, sampler = thedesignA,
          samples = 1000)

                        Value Pr(>x)
Estadística de Prueba Observada     1   0.32
```



## Cómo hacer esto en  R: RI2

¿Cómo deberíamos interpretar el valor $p$ de dos colas aquí?

```{r,echo=TRUE}
## usando el paquete ri2
library(ri2)
thedesign <- declare_ra(N = N)
dat$Z <- dat$T
pMean4 <- conduct_ri(Y ~ Z,
  declaration = thedesign,
  sharp_hypothesis = 0, data = dat, sims = 1000
)
summary(pMean4)
pMeanRank4 <- conduct_ri(rankY ~ Z,
  declaration = thedesign,
  sharp_hypothesis = 0, data = dat, sims = 1000
)
summary(pMeanRank4)
```

## Siguientes temas

 - Probando hipótesis nulas débiles, $H_0: \bar{y}_{1} = \bar{y}_{0}$.

 - Rechazando hipótesis nulas (y cometiendo errores de falsos positivos y/o falsos negativos).

 - Mantener las tasas de error de falsos positivos correctas cuando se evalúa más de una hipótesis.

- Poder de las pruebas de hipótesis ([Modulo sobre el Poder Estadístico y los Diagnósticos de Diseño](https://egap.github.io/learningdays-book/statistical-power-and-design-diagnosands.html)).


# Probando hipótesis nulas débiles

## Probando las hipótesis nulas débiles de que no hay efectos promedio

- La hipótesis nula débil es una afirmación sobre los agregados, y casi siempre se plantea en términos de promedios: $H_0: \bar{y}_{1} = \bar{y}_{0}$

- La estadística de prueba para esta hipótesis es casi siempre la diferencia simple de medias (por ejemplo, el `meanTT()` anteriormente mencionado).

```{r simpdiffs, echo=TRUE}
lm1 <- lm(Y ~ T, data = dat)
lm1P <- summary(lm1)$coef["T", "Pr(>|t|)"]
ttestP1 <- t.test(Y ~ T, data = dat)$p.value
library(estimatr)
ttestP2 <- difference_in_means(Y ~ T, data = dat)
c(lm1P = lm1P, ttestP1 = ttestP1, tttestP2 = ttestP2$p.value)
```

- ¿Por qué el valor $p$ de OLS es diferente? ¿Qué supuestos utilizamos para calcularlo?

## Probando las hipótesis nulas débiles de que no hay efectos promedio

Tanto la variación como la ubicación de $Y$ cambia con el tratamiento en esta simulación.

```{r fig.cap="Boxplot  de las variables de resultado observados por el estado de tratamiento", results='asis', out.width=".7\\textwidth"}
boxplot(Y ~ T, data = dat)
```


## Probando las hipótesis nulas débiles de que no hay efectos promedio

```{r, echo=TRUE}
## By hand:
varEstATE <- function(Y, T) {
  var(Y[T == 1]) / sum(T) + var(Y[T == 0]) / sum(1 - T)
}
seEstATE <- sqrt(varEstATE(dat$Y, dat$T))
obsTStat <- observedMeanTT / seEstATE
c(
  observedTestStat = observedMeanTT,
  stderror = seEstATE,
  tstat = obsTStat,
  pval = 2 * min(
    pt(obsTStat, df = 8, lower.tail = TRUE),
    pt(obsTStat, df = 8, lower.tail = FALSE)
  )
)
```

# Rechazando hipótesis nulas

## Rechazando hipótesis y creando errores

- "Típicamente, el nivel de una prueba [$\alpha$] es una promesa sobre el rendimiento de esta, el tamaño es un dato sobre su rendimiento…" (Rosenbaum 2010, Glosario)

- $\alpha$ es la probabilidad de rechazar la hipótesis nula cuando la hipótesis nula es verdadera.

- ¿Cómo deberíamos interpretar $p$=`r  round(pMeanTT,2)`? ¿Cómo deberíamos interpretar $p$=`r round(pMeanRankTT,2)` (nuestras pruebas de la hipótesis nula tajante)?

- ¿Qué significa “rechazar” $H_0: y_{i,1}=y_{i,2}$ at $\alpha=.05$?



## Tasas de falsos positivos en las pruebas de hipótesis {.allowframebreaks}

```{r normp, echo=FALSE,out.width=".5\\textwidth",fig.cap="Valor p de una prueba estadística con una distribución normal."}
library(tidyverse)
ggplot(NULL, aes(c(-3, 3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(2, 3)) +
  geom_area(stat = "function", fun = dnorm, fill = "grey80", xlim = c(-3, 2)) +
  labs(x = "test stat (center=0)", y = "prob") +
  geom_vline(xintercept = 2) +
  scale_y_continuous(breaks = NULL) +
  # scale_x_continuous(breaks = 4) +
  theme_classic()
```

Nótese que:

- La curva está centrada en el valor hipotético.
 
 - La curva representa el mundo de la hipótesis.
 
 - El valor $p$ es lo raro que sería ver la estadística de prueba observada (o un valor más alejado del valor hipotético) en el mundo de la hipótesis nula.
 
- En la imagen, el valor observado de la estadística de prueba es consistente con la distribución hipotetizada, pero no es súper consistente.
 
 - Incluso si $p < .05$ (o $p < .001$), la estadística de prueba observada debe reflejar algún valor de la distribución hipotetizada. Esto significa que siempre se puede cometer un error al rechazar una hipótesis nula.

## Errores de falsos positivos y falsos negativos 

- Si decimos: "¡El resultado experimental es significativamente diferente del
valor hipotético de cero ($p=.001$)! ¡Rechazamos la hipótesis!” **cuando la verdad es cero** estamos cometiendo un **error de falso positivo** (pretender detectar algo positivamente cuando no hay señal, sólo ruido).

- Si decimos: "No podemos distinguir este resultado del cero ($p=.3$). No podemos rechazar la hipótesis de cero". **cuando la verdad no es cero** estamos cometiendo un **error de falso negativo** (afirmando la incapacidad de detectar algo cuando hay una señal, pero está abrumada por el ruido).

## Una sola prueba de una sola hipótesis

- Una prueba de una unica
 hipótesis debería fomentar los errores de falsos positivos en contadas ocasiones (por ejemplo, si establecemos $\alpha=.05$), entonces estamos diciendo que nos sentimos cómodos con que nuestro procedimiento de prueba cometa errores de falsos positivos en **no más del 5% de las pruebas de una asignación de tratamiento dada en un experimento determinado**.

- Además, una **prueba única de una sola hipótesis** debería detectar la señal cuando existe --- debería tener un alto **poder estadístico**. En otras palabras, no debería fallar en la detección de una señal cuando existe (es decir, debería tener bajas tasas de error de falso negativo).

## Las decisiones implican errores

- Si los errores son inevitables, ¿cómo podemos diagnosticarlos? ¿Cómo podemos saber si nuestro proceso de comprobación de hipótesis puede generar demasiados errores de falsos positivos?

- ¡Diagnosticar usando simulación!

## Diagnosticar las tasas de falsos positivos mediante simulación

- A través de repeticiones del diseño:

  - Cree una hipótesis nula verdadera.
  - Pruebe la hipótesis verdadera nula.
  - El valor $p$ debe ser grande si la prueba funciona correctamente.

- La proporción de valores $p$ pequeños no debería ser mayor que $\alpha$ si la prueba funciona correctamente.

## Diagnosticar las tasas de falsos positivos mediante simulación

Ejemplo con un resultado binario.  ¿Funciona la prueba cómo debería? 

##  \small Diagnosticar falsos positivos mediante simulación

\small ¿Cómo son los valores p cuándo no hay efecto?

```{r, echo=TRUE}
collectPValues <- function(y, trt, thedistribution = exact()) {
  new_trt <- repeatExperiment(length(y))  ## Y y T no tienen relación 
  thedata <- data.frame(new_trt = new_trt, y = y)
  thedata$ranky <- rank(y)
  thedata$new_trtF <- factor(thedata$new_trt)
  thelm <- lm(y ~ new_trt, data = thedata)   ## Las cuatro pruebas 
  t_test_CLT <- difference_in_means(y ~ new_trt, data = thedata)
  t_test_exact <- oneway_test(y ~ new_trtF, data = thedata,
    distribution = thedistribution)
  t_test_rank_exact <- oneway_test(ranky ~ new_trtF, data = thedata,
    distribution = thedistribution)
  owP <- coin::pvalue(t_test_exact)[[1]]
  owRankP <- coin::pvalue(t_test_rank_exact)[[1]]  
  return(c( ## Regrese los valores p
    lmp = summary(thelm)$coef["new_trt", "Pr(>|t|)"],
    neyp = t_test_CLT$p.value[[1]],
    rtp = owP,
    rtpRank = owRankP))}
```

```{r fprdsim, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
set.seed(123456)
theY <- dat$Ybin
theT <- dat$T
pDist <- replicate(5000, collectPValues(y = theY, trt = theT))
```

## Diagnosticar las tasas de falsos positivos mediante simulación

- Cuando no hay ningún efecto, una prueba de la hipótesis nula de ausencia de efectos debería
producir un valor p **grande**. 

- Si la prueba funciona bien, deberíamos ver sobre todo valores p grandes y muy pocos valores p pequeños.

- Algunos de los valores p para las cuatro pruebas diferentes (hicimos 5000 simulaciones, sólo mostramos 5)

```{r, echo=FALSE}
pDist[, 1:5]
```

## Diagnosticar las tasas de falsos positivos mediante simulación

De hecho, si no hay ningún efecto, y si decidimos rechazar la hipótesis nula
de ausencia de efectos con $\alpha=.25$, querríamos que **no más del 25% de nuestros
valores p en esta simulación sean menores que p=.25**. ¿Qué podemos observar aquí? ¿Qué
pruebas parecen tener tasas de falsos positivos demasiado altas?

```{r pdistsummary, echo=TRUE}
## Calcule la proporción de valores p menores que .25, por cada fila de pDist
apply(pDist, 1, function(x) {
  mean(x < .25)
})
```


## Diagnosticar las tasas de falsos positivos mediante simulación

Compare las pruebas trazando la proporción de valores p menores que un número determinado. Las pruebas de "inferencia aleatoria" controlan la tasa de falsos positivos (son las pruebas de uso de permutación directa, repitiendo el experimento).


```{r plotecdf, results='asis', echo=FALSE, message=FALSE, warning=FALSE, fig.cap=  ' \\tiny Dist. p-val para 4 pruebas cuando no hay efectos n=10. Cuando la prueba controla su tasa de falsos positivos los puntos caen sobre o por debajo de la línea diagonal.' ,out.width='.6\\textwidth'}
par(mfrow = c(1, 1), mgp = c(2, .75, 0), oma = rep(0, 4), mar = c(3.5, 3.5, 0, 0))
plot(c(0, 1), c(0, 1),
  type = "n",
  xlab = "valor p=p", ylab = "Proporción de valores p < p",
  cex.lab = 2, cex.axis = 2
)
for (i in 1:nrow(pDist)) {
  lines(ecdf(pDist[i, ]), pch = i, col = i, cex = 2, cex.axis = 2, lwd = 2)
}
abline(0, 1, col = "gray")
legend("topleft",
  legend = c("OLS", "Neyman", "Rand Inf Mean Diff", "Rand Inf Mean Diff Ranks"),
  pch = 1:5, col = 1:5, lty = 1, bty = "n", cex = 2
)
```

## Tasa de falsos positivos con $N=60$ y variable de resultado binaria

En este diseño, sólo las pruebas basadas en la inferencia de aleatorización directa controlan la tasa de falsos positivos.

```{r fprdsimBig, cache=TRUE}
set.seed(12345)
## pDistBig <- replicate(1000,collectPValues(y=bigdat$Ybin,z=bigdat$T,thedistribution=approximate(B=1000)))
library(parallel)
pDistBigLst <- mclapply(1:1000, function(i) {
  collectPValues(y = bigdat$Ybin, trt = bigdat$T, thedistribution = approximate(nresample = 1000))
}, mc.cores = 8)
pDistBig <- simplify2array(pDistBigLst)
```

```{r plotecdfBig, results='asis', echo=FALSE, message=FALSE, warning=FALSE,out.width='.7\\textwidth',fig.cap=" \\tiny  Dist. P-val para 4 pruebas cuando no hay efectos n=60 y una variable de resultado binaria. Una prueba que controla la tasa de falsos positivos debería tener puntos en la línea diagonal o por debajo de ella."}
par(mfrow = c(1, 1), mgp = c(2, .75, 0), oma = rep(0, 4), mar = c(3.5, 3.5, 0, 0))
plot(c(0, 1), c(0, 1),
  type = "n",
  xlab = "p-value=p", ylab = "Proporción de valores p < p",
  cex.lab = 2, cex.axis = 2
)
for (i in 1:nrow(pDistBig)) {
  lines(ecdf(pDistBig[i, ]), pch = i, col = i, cex = 2, cex.axis = 2, lwd = 2)
}
abline(0, 1, col = "gray")
legend("topleft",
  legend = c("OLS", "Neyman", "Rand Inf Mean Diff", "Rand Inf Mean Diff Ranks"),
  pch = 1:5, col = 1:5, lty = 1, bty = "n", cex = 2
)
```

## Tasa de falsos positivos con $N=60$ y variable de resultado continua

Aquí todas las pruebas hacen un buen trabajo al controlar la tasa de falsos positivos.


```{r fprdsimBig2, cache=TRUE}
set.seed(123456)
pDistBigLst2 <- mclapply(1:1000, function(i) {
  collectPValues(y = bigdat$Y, trt = bigdat$T, thedistribution = approximate(nresample = 1000))
}, mc.cores = 8)
pDistBig2 <- simplify2array(pDistBigLst2)
```

```{r plotecdfBig2, results='asis', echo=FALSE, message=FALSE, warning=FALSE, out.width='.7\\textwidth',fig.cap= '\\tiny  Dist. P-val para 4 pruebas cuando no hay efectos  n=60 y una variable de resultado continua. Cuando la  prueba que controla la tasa de falsos positivos los puntos caen sobre o debajo de la línea diagonal.'}
library(scales)
par(mfrow = c(1, 1), mgp = c(2, .75, 0), oma = rep(0, 4), mar = c(3.5, 3.5, 0, 0))
plot(c(0, 1), c(0, 1),
  type = "n",
  xlab = "p-value=p", ylab = "Proporción de valores < p",
  cex.lab = 2, cex.axis = 2
)
for (i in 1:nrow(pDistBig2)) {
  lines(ecdf(pDistBig2[i, ]), pch = i, col = alpha(i, .5), cex = 2, cex.axis = 2)
}
abline(0, 1, col = "gray")
legend("topleft",
  legend = c("OLS", "Neyman", "Rand Inf Mean Diff", "Rand Inf Mean Diff Ranks"),
  pch = 1:5, col = 1:5, lty = 1, bty = "n", cex = 2
)
```


## Resumen

- Una buena prueba: 

    1. raramente arroja dudas sobre la verdad, y 
    
    2. distingue fácilmente la señal del ruido (pone en duda la falsedad con frecuencia).

- Podemos saber si nuestro procedimiento de prueba controla las tasas de falsos positivos dado nuestro diseño.

- Cuando las tasas de falsos positivos no están controladas, ¿qué puede estar fallando? (A menudo tiene que ver con las asintóticas).

# Temas avanzados

## Algunos temas avanzados relacionados con las pruebas de hipótesis

 - Incluso si un procedimiento de prueba determinado controla la tasa de falsos positivos para una sola prueba, puede que no controle la tasa para un grupo de pruebas múltiples. Léase:
[10 Things you need to know about multiple
   comparisons](https://egap.org/methods-guides/10-things-you-need-know-about-multiple-comparisons)
   para obtener una guía de los enfoques para controlar dichas tasas de rechazo en múltiples pruebas.

 - Un intervalo de confianza de $100(1-\alpha)$ puede definirse como el rango de hipótesis en el que todos los valores $p$ son mayores o iguales que $alpha$. Esto se llama invertir la prueba de hipótesis (@rosenbaum2010design). Es decir, un intervalo de confianza es una colección de pruebas de hipótesis.

## Qué más debe saber sobre las pruebas de hipótesis {.allowframebreaks}

 - Una estimación puntual basada en pruebas de hipótesis se denomina estimación puntual de Hodges-Lehmann (@rosenbaum1993hlp,@hodges1963elb).

 - Un conjunto de pruebas de hipótesis puede combinarse en una sola prueba de hipótesis (@hansen:bowers:2008,@caughey2017nonparametric).

 - En las pruebas de equivalencia, se puede plantear la hipótesis de que dos estadísticas de prueba son equivalentes (es decir, el grupo de tratamiento es el mismo que el grupo de control) en lugar de una sola estadística de prueba (la diferencia entre los dos grupos es cero) (@hartman2018equivalence).

 - Dado que una prueba de hipótesis es un modelo de variable de resultado potenciales, se puede utilizar la prueba de hipótesis para aprender sobre modelos complejos, como los modelos de propagación de los efectos del tratamiento a través de redes (@bowers2013reasoning, @bowers2016research, @bowers2018models)





## Ejercicio: Pruebas de hipótesis y estadísticas de prueba

 1. Si una intervención fue muy eficaz en el aumento de la variabilidad de una variable de resultado, pero no cambió la media, ¿sería grande o pequeño el valor $p$ reportado por R o Stata si utilizamos `lm_robust()` o `difference_of_means()` o `reg` o `t.test`?

 2. Si una intervención hace que la media del grupo de control se reduzca moderadamente pero aumenta mucho algunas variables de resultado (como un efecto 10 veces mayor), ¿el valor $p$ de R `lm_robust()` o `difference_of_means()` sería grande o pequeño?


# Probar muchas hipótesis

## ¿Cuándo podríamos probar muchas hipótesis?

- ¿Difiere el efecto de un tratamiento experimental entre los distintos grupos? ¿Podrían surgir diferencias en el efecto del tratamiento debido a algunas característicasde los sujetos experimentales?

- ¿Qué estrategias de comunicación fueron más eficaces en una única variable de resultado?

- ¿Cuáles, entre varias variables de resultado, fueron influenciados por una única intervención experimental?


## Tasas de falsos positivos en las pruebas de hipótesis múltiples {.fragile}

Digamos que nuestra probabilidad de cometer un error de falso positivo es de 0,05 en una sola prueba. ¿Qué ocurre si preguntamos: (1) *cuál de estos 10 resultados tiene una relación estadísticamente significativa con los dos brazos de tratamiento*? o (2) *cuál de estos 10 brazos de tratamiento tiene una relación estadísticamente significativa con la única variable de resultado*?

 - La probabilidad de un error de falso positivo debe ser menor o igual a 0,05 en una prueba.
 - La probabilidad de un error de falso positivo debe ser menor o igual a  $1 - ( ( 1 - .05 ) \times (1 - .05) ) = .0975$ en 2 pruebas.
 - La probabilidad de al menos un error de falso positivo con $\alpha=.05$ en 10 pruebas debería ser $\le$ $1 - (1-.05)^{10}=.40$.

## Descubrimientos con pruebas múltiples

**Número de errores hechos al probar hipótesis nulas $m$** [@benjamini1995
's Table 1]. Las celdas son el número de pruebas.  $R$ es # de "descubrimientos" y $V$ es # de falsos descubrimientos, $U$ es # de no rechazos
correctos, y $S$ es # de rechazos correctos.




+---------------------------------------+--------------------------+----------------------+-----------+
|                                       | Declarado                 | Declarado           |   Total   |
|                                       | No significativo          | Significativo       |           |
+=======================================+:========================:+:====================:+:=========:+
| Hipótesis nula real ($H_{real}=0$)    |             U            |           V          |   $m_0$   |
+---------------------------------------+--------------------------+----------------------+-----------+
| Hiptsis nula falsa ($H_{real} \ne 0$) |             T            |           S          | m - $m_0$ |
+---------------------------------------+--------------------------+----------------------+-----------+
| Total                                 |            m-R           |           R          |    m      |
+---------------------------------------+--------------------------+----------------------+-----------+








## Dos tasas de error principales a controlar cuando se prueban muchas hipótesis {.allowframebreaks}

1. **La tasa de error familiar (FWER)** es $P(V>0)$ (Probabilidad de cualquier error de falso
       positivo).
      
    - Nos gustaría controlar esto si planeamos tomar una decisión sobre los resultados de     nuestras pruebas múltiples. El proyecto de investigación es principalmente confirmatorio. 
       
    - Véanse, por ejemplo, los proyectos de la OES
       <http://oes.gsa.gov>: las agencias federales tomarán decisiones sobre
       programas en función de la detección de resultados o no.
             
2. **La tasa de falsos descubrimientos (FDR)** es $E(V/R | R>0)$ (proporción promedio de
       Errores de falsos positivos dados algunos rechazos). 
   
    - Nos gustaría controlar esto si estamos usando *este* experimento para planificar *el próximo* experimento. Estamos dispuestos a aceptar una mayor probabilidad de error en aras de darnos más posibilidades de descubrimiento. 

    - Por ejemplo, se podría
       imaginar que una organización, un gobierno, una ONG, podría decidir llevar a cabo
       *una serie de experimentos* como parte de una "agenda de aprendizaje": ningún experimento determina la toma de decisiones, hay más espacio para la exploración.

Nos centraremos en el FWER, pero recomendamos pensar en el FDR y en las agendas de aprendizaje como una forma muy útil de proceder.


## Preguntas con variables de resultado múltiples

- ¿Cuál es el efecto de un tratamiento sobre múltiples variables de resultado? 

- ¿En qué variables de resultado (de entre muchas) tuvo efecto el tratamiento? 

- La segunda pregunta, en particular, puede conducir al tipo de problemas de tasa de error familiar relacionadas entre sí a los que nos referimos anteriormente.

## Pruebas de hipótesis múltiples: Variables de Resultado Múltiples 

Imagine que tenemos cinco variables de resultado y un tratamiento (mostrando aquí las variables de resultado potenciales y observadas):

```{r multtesting1}
set.seed(23)
thedat <- fabricate(
  N = 100,
  y0_1 = rnorm(N),
  y0_2 = rnorm(N),
  y0_3 = rnorm(N),
  y0_4 = rnorm(N),
  y0_5 = rnorm(N)
)
tau1 <- 0
tau5 <- tau4 <- tau3 <- tau2 <- tau1
thepop <- declare_population(thedat)
theassign <- declare_assignment(T = complete_ra(N = N, m = 50))
po_1 <- declare_potential_outcomes(Y1_T_0 = y0_1, Y1_T_1 = y0_1 + tau1)
po_2 <- declare_potential_outcomes(Y2_T_0 = y0_2, Y2_T_1 = y0_2 + tau2)
po_3 <- declare_potential_outcomes(Y3_T_0 = y0_3, Y3_T_1 = y0_3 + tau3)
po_4 <- declare_potential_outcomes(Y4_T_0 = y0_4, Y4_T_1 = y0_4 + tau4)
po_5 <- declare_potential_outcomes(Y5_T_0 = y0_5, Y5_T_1 = y0_5 + tau5)
reveal_1 <- declare_reveal(Y1, T)
reveal_2 <- declare_reveal(Y2, T)
reveal_3 <- declare_reveal(Y3, T)
reveal_4 <- declare_reveal(Y4, T)
reveal_5 <- declare_reveal(Y5, T)

des1 <- thepop + theassign +
  po_1 + po_2 + po_3 + po_4 + po_5 +
  reveal_1 + reveal_2 + reveal_3 + reveal_4 + reveal_5

dat1 <- draw_data(des1)
options(digits = 2)
head(dat1[, -c(2:6, 18:22)])
head(dat1[, c(1, 7, 18:22)])
```

## Podemos detectar un efecto en la variable de resultado `Y1`?

¿Podemos detectar un efecto en la variable de resultado `Y1`? (es decir, ¿la prueba de hipótesis produce un valor $p$ lo suficientemente pequeño?)
```{r p1, echo=TRUE}
coin::pvalue(oneway_test(Y1 ~ factor(T), data = dat1))
## Notese que el valor p de la prueba t es igual a la prueba chi al cuadrado
## valor p.
coin::pvalue(independence_test(Y1 ~ factor(T),
  data = dat1,
  teststat = "quadratic"
))
```

## ¿En cuál de las cinco variables de resultado podemos detectar un efecto?

¿En cuál de las cinco variables de resultado podemos detectar un efecto? (es decir, ¿alguna de las cinco pruebas de hipótesis produce un valor $p$ lo suficientemente pequeño?)
```{r pmult, echo=TRUE}
p1 <- coin::pvalue(oneway_test(Y1 ~ factor(T), data = dat1))
p2 <- coin::pvalue(oneway_test(Y2 ~ factor(T), data = dat1))
p3 <- coin::pvalue(oneway_test(Y3 ~ factor(T), data = dat1))
p4 <- coin::pvalue(oneway_test(Y4 ~ factor(T), data = dat1))
p5 <- coin::pvalue(oneway_test(Y5 ~ factor(T), data = dat1))
theps <- c(p1 = p1, p2 = p2, p3 = p3, p4 = p4, p5 = p5)
sort(theps)
```


## ¿Podemos detectar un efecto en “cualquiera” de las cinco variables de resultado? 

¿Podemos detectar un efecto en “cualquiera” de las cinco variables de resultado? (es decir, ¿ pueden las cinco pruebas de hipótesis para las cinco variables de resultado producir un valor $p$ lo suficientemente pequeño?)
```{r omnibus, echo=TRUE}
coin::pvalue(independence_test(Y1 + Y2 + Y3 + Y4 + Y5 ~ factor(T),
  data = dat1, teststat = "quadratic"
))
```

¿Cuál enfoque es más probable que nos engañe con demasiados resultados "estadísticamente significativos" (5 pruebas o 1 prueba ómnibus)?

## Comparación de enfoques I

Hagamos una simulación para conocer estos enfoques de prueba.

- Vamos a (1) establecer que los verdaderos efectos causales sean 0, (2) reasignar repetidamente el tratamiento, y (3) hacer cada una de estas tres pruebas cada vez.

- Dado que el efecto verdadero es 0, esperamos que la *mayoría* de los valores $p$ sean grandes. (De hecho, nos gustaría que no más del 5% de
los valores $p$ sean mayores que $p=.05$, si utilizamos el criterio de aceptación-rechazo $\alpha=.05$).

```{r testsetup1, echo=FALSE, results="hide"}
ttest_Y1fn <- function(data) {
  p1 <- coin::pvalue(oneway_test(Y1 ~ factor(T), data = data))
  return(data.frame(statistic = NA, p.value = p1))
}
ttest_multfn <- function(data) {
  p1 <- coin::pvalue(oneway_test(Y1 ~ factor(T), data = data))
  p2 <- coin::pvalue(oneway_test(Y2 ~ factor(T), data = data))
  p3 <- coin::pvalue(oneway_test(Y3 ~ factor(T), data = data))
  p4 <- coin::pvalue(oneway_test(Y4 ~ factor(T), data = data))
  p5 <- coin::pvalue(oneway_test(Y5 ~ factor(T), data = data))
  theps <- c(p1, p2, p3, p4, p5)
  return(data.frame(statistic = NA, p.value = min(theps)))
}
ttest_mult_holmfn <- function(data) {
  p1 <- coin::pvalue(oneway_test(Y1 ~ factor(T), data = data))
  p2 <- coin::pvalue(oneway_test(Y2 ~ factor(T), data = data))
  p3 <- coin::pvalue(oneway_test(Y3 ~ factor(T), data = data))
  p4 <- coin::pvalue(oneway_test(Y4 ~ factor(T), data = data))
  p5 <- coin::pvalue(oneway_test(Y5 ~ factor(T), data = data))
  theps <- c(p1, p2, p3, p4, p5)
  padj <- p.adjust(theps, method = "holm")
  minp <- min(padj)
  return(data.frame(statistic = NA, p.value = minp))
}
ttest_omnibusfn <- function(data) {
  thep <- coin::pvalue(independence_test(Y1 + Y2 + Y3 + Y4 + Y5 ~ factor(T), data = data, teststat = "quadratic"))
  return(data.frame(statistic = NA, p.value = thep))
}

ttest_Y1 <- declare_test(handler = label_test(ttest_Y1fn), label = "t-test Y1")
ttest_mult <- declare_test(handler = label_test(ttest_multfn), label = "t-test all")
ttest_mult_holm <- declare_test(handler = label_test(ttest_mult_holmfn), label = "t-test all holm adj")
ttest_omnibus <- declare_test(handler = label_test(ttest_omnibusfn), label = "t-test omnibus")
ttest_Y1(dat1)
ttest_mult(dat1)
ttest_mult_holm(dat1)
ttest_omnibus(dat1)
```


```{r des1setup,echo=FALSE}
des1_plus <- des1 + ttest_Y1 + ttest_mult + ttest_mult_holm + ttest_omnibus
thediagnosands <- declare_diagnosands(fwer = mean(p.value < .05))
### Errores sobre buscar un estimado
## des1_diag <- diagnose_design(design = des1_plus, bootstrap_sims = 0,
##     sims=10, diagnosands = thediagnosands)
```

```{r dd1, echo=TRUE, cache=TRUE}
des1_sim <- simulate_design(des1_plus, sims = 1000)
res1 <- des1_sim %>%
  group_by(estimator) %>%
  summarize(fwer = mean(p.value < .05), .groups = "drop")
```

## Comparación de enfoques II

```{r}
kableExtra::kable(res1, caption = "Tasas de error familiares")
```

- El enfoque que utiliza 5 pruebas produce una $p < .05$ con demasiada frecuencia ---
recuerde que no hay efectos causales en absoluto para ninguno de estas variables de resultado. 

 - Una prueba de una sola variable de resultado (`Y1`) tiene $p < .05$ en no más del 5% de las simulaciones.

 - La prueba ómnibus también muestra una tasa de error bien controlada.

 - El uso de una corrección de pruebas múltiples (aquí utilizamos la corrección de "Holm") también controla correctamente la tasa de falsos positivos.

## La corrección de Holm

Así es como se usa la corrección de Holm (Note lo que le sucede a los a los valores $p$):

```{r holmex, echo=TRUE}
theps
p.adjust(theps, method = "holm")
## Para mostrar que sucede con los valores p "significativos"
theps_new <- sort(c(theps, newlowp = .01))
p.adjust(theps_new, method = "holm")
```


## Pruebas de hipótesis múltiples: Brazos de tratamiento múltiples {.allowframebreaks}

- El mismo tipo de problema puede darse cuando la pregunta es sobre los efectos diferenciales
de un tratamiento con múltiples brazos.

- Con 5 brazos, "el efecto del brazo 1" podría significar muchas cosas diferentes: "¿Es la variable de resultado potencial promedio del brazo 1 más grande que la del brazo 2?", "¿Son las variables de resultado potenciales del brazo 1 más grandes que el promedio de las variables de resultado potenciales de todos los demás brazos?".

- ¡Si sólo nos enfocamos en las comparaciones por pares entre brazos, podríamos tener $((5 \times 5) - 5)/2 = 10$ pruebas únicas!

## Pruebas de hipótesis múltiples: Brazos de tratamiento múltiples {.allowframebreaks}

Aquí hay unas potenciales variables de resultado observadas, y unos valores múltiples de `T`.

```{r multitreatsetup, echo=FALSE}
theassign_mult <- declare_assignment(T = conduct_ra(N = N, num_arms = 5, conditions = c("1", "2", "3", "4", "5")))
po_mult <- declare_potential_outcomes(Y ~ y0_1 * (T == "1") + y0_2 * (T == "2") +
  y0_3 * (T == "3") + y0_4 * (T == "4") + y0_5 * (T == "5"),
conditions = c("1", "2", "3", "4", "5"),
assignment_variables = T
)
reveal_mult <- declare_reveal(assignment_variables = T)
des2 <- thepop + theassign_mult + po_mult + reveal_mult
dat2 <- draw_data(des2)
options(digits = 2)
## T es el brazo de tratamiento: 1,2,3,4,5
head(dat2[, -c(2:6, 8)])
options(digits = 4)
```

## Pruebas de hipótesis múltiples: Brazos de tratamiento múltiples {.allowframebreaks}


Aquí están las 10 pruebas por pares con y sin ajuste para pruebas múltiples. Observe cómo un resultado "significativo" ($p=.01$) cambia con el ajuste.

```{r}
## esta es una interface para la prueba de independencia de coin coin's independence_test()
pair_tests1 <- pairwisePermutationTest(Y ~ T, data = dat2, distribution = asymptotic(), method = "holm", teststat = "quadratic")
pair_tests1
```
## Enfoques para la prueba de hipótesis con múltiples brazos

Mostramos cuatro enfoques diferentes:

  1. Hacer todas las pruebas por pares y elegir la mejor (mala idea);
  2. Hacer todas las pruebas por pares y elegir la mejor después de ajustar
los valores p para las pruebas múltiples (buena idea, pero con muy poco
poder estadístico);
  3. probar la hipótesis de que no hay relación entre *cualquier brazo* (una prueba ómnibus)
y la variable de resultado (buena idea);
  4. elegir un brazo en el que concentrarse de antemano (buena idea).

```{r}
ttest_T1_vs_allfn <- function(data) {
  p1 <- coin::pvalue(oneway_test(Y ~ factor(T == "1"), data = data))
  return(data.frame(statistic = NA, p.value = p1))
}
overall_Tfn <- function(data) {
  itest <- independence_test(Y ~ T, data = data)
  return(data.frame(statistic = NA, p.value = coin::pvalue(itest)))
}
pairwise_testsfn <- function(data) {
  pair_tests <- pairwisePermutationTest(Y ~ T, data = data, distribution = asymptotic(), method = "holm", teststat = "quadratic")
  return(data.frame(statistic = NA, p.value = min(pair_tests$p.value)))
}
pairwise_tests_adjfn <- function(data) {
  pair_tests <- pairwisePermutationTest(Y ~ T, data = data, distribution = asymptotic(), method = "holm", teststat = "quadratic")
  return(data.frame(statistic = NA, p.value = min(pair_tests$p.adjust)))
}


## dat2$TF <- factor(dat2$T)
## blah <- independence_test(Y~TF,data=dat2,xtrafo = mcp_trafo(TF = "Tukey")) #,teststat="quadratic")
## pair_tests2 <- coin::pvalue(blah,method="unadjusted")
## pair_tests2
## thecontrasts <- rbind("2 - 1 " = c(1,-1,0,0,0,0),
## "3-1"=c(1,0,-1,0,0,0))
## blah2<- independence_test(Y~TF,data=dat2,xtrafo = mcp_trafo(TF = thecontrasts))
## coin::pvalue(blah2,method="single-step")
##

ttest_T1_vs_all <- declare_test(handler = label_test(ttest_T1_vs_allfn), label = "prueba t T1 vs todas")
overall_T <- declare_test(handler = label_test(overall_Tfn), label = "Prueba general")
pairwise_test <- declare_test(handler = label_test(pairwise_testsfn), label = "Escoger mejor prueba en parejas")
pairwise_test_adj <- declare_test(handler = label_test(pairwise_tests_adjfn), label = "Escoger mejor prueba en parejas después de ajuste")
```

```{r des2diag, cache=TRUE}
des2_plus <- des2 + ttest_T1_vs_all + overall_T + pairwise_test + pairwise_test_adj
thediagnosands <- declare_diagnosands(fwer = mean(p.value < .05))
des2_sim <- simulate_design(des2_plus, sims = 1000)
res2 <- des2_sim %>%
  group_by(estimator) %>%
  summarize(fwer = mean(p.value < .05), .groups = "drop")
kableExtra::kable(res2, caption = "Enfoques para pruebas en experimentos de múltiples brazos.")
```

## Resumen

- Los problemas en las pruebas múltiples pueden surgir de múltiples variables de resultado o múltiples tratamientos (o múltiples moderadores/términos de interacción).
  
- Los procedimientos para realizar pruebas de hipótesis e intervalos de confianza pueden implicar errores. La práctica ordinaria controla las tasas de error en una sola prueba (o en un solo intervalo de confianza). Pero las pruebas múltiples requieren  trabajo adicional para garantizar que las tasas de error estén controladas.

- La pérdida de poder derivada de los enfoques de ajuste nos obliga a considerar qué *preguntas queremos hacer a los datos*. Por ejemplo, si queremos saber si el tratamiento tuvo *algún efecto*, una prueba conjunta o una prueba ómnibus con múltiples variables de resultado aumentará nuestro poder estadístico sin necesidad de ajustes.


# Referencias  {.allowframebreaks}


