---
title: "Tests d'hypothèses: résumer les informations sur les effets causals"
author: "Mettre votre nom"
date: "`r format(Sys.time(), '%d-%m-%Y')`"
bibliography: ../learningdays-book.bib
biblio-style: apalike
link-citations: yes
colorlinks: yes
fig_caption: yes
header-includes: |
   \setbeamertemplate{footline}{\begin{beamercolorbox}{section in head/foot}
   \includegraphics[height=.5cm]{../Images/egap-logo.png} \hfill
   \insertframenumber/\inserttotalframenumber \end{beamercolorbox}}
   \usepackage{tikz}
   \usepackage{tikz-cd}
   \usepackage{textpos}
   \usepackage{booktabs,multirow,makecell}
output:
  revealjs::revealjs_presentation:
    fig_caption: true
    theme: default
    highlight: pygments
    center: false
    transition: fade
    smart: false
    self_contained: false
    reveal_plugins: ["notes", "search", "chalkboard"]
    pandoc_args: [ "--toc" ]
    reveal_options:
      slideNumber: true
      previewLinks: true
      chalkboard:
        theme: whiteboard
        toggleNotesButton: false
  beamer_presentation:
    keep_tex: true
    toc: true
    pandoc_args: [ "--toc" ]
    fig_caption: true
---

```{r setup, include=FALSE}
source("rmd_setup.R")
# Load all the libraries we need
library(here)
library(tidyverse)
library(kableExtra)
library(DeclareDesign)
library(estimatr)
library(styler)
library(coin)
library(multcomp)
library(devtools)
library(randomizr)
library(rcompanion) ## for pairwisePermutationTest()
```
# Le rôle des tests d'hypothèses dans l'inférence causale

## Points principaux

- Inférence statistique (e.g., les tests d'hypothèses et les intervalles de confiance) nécessitent **l'inférence** --- i.e. raisonner sur ce qui n'a pas été observé.

- $p$-valeurs nécessitent des distributions de probabilité .

- Randomisation (ou Design) + une hypothèse + une function statistique de test $\rightarrow$ distributions de probabilité représentant l'hypothèse (distributions de référence)

- Valeurs observées des tests statistiques + distribution de référence $\rightarrow$ $p$-valeur.

## Le role des tests d'hypothèses dans l'inférence causale I

- Le **problème fondamental de l'inférence causale** dit que nous ne pouvons voir qu'un seul résultat potentiel pour une unité donnée.

- Donc, si un effet causal contrefactuel du traitement, $T$, pour Dupont se produit lorsque $y_{\text{Dupont},T=1} \ne y_{\text{Dupont},T=0}$, alors comment pouvons-nous en savoir plus sur l'effet causal ?

- Une solution est l'**[estimation](estimation-slides.Rmd) des moyennes des effets causaux** (les ATE, ITT, LATE).

- C'est l'approche de Neyman.

## Le role des tests d'hypothèses dans l'inférence causale II

- Une autre solution consiste à faire des **affirmations** ou **des suppositions** sur les effets causaux.

- On pourrait dire : "Je pense que l'effet sur Dupont est de 5". ou "Cette expérience n'a eu d'effet sur personne." Et puis nous demandons "Quelles preuves apporte cette expérience à propos de cette affirmation ?"

- La preuve est contenue dans la $p$-valeur .

-  C'est l'approche de Fisher.

## Le role des tests d'hypothèses dans l'inférence causale III

- L'approche du test d'hypothèse pour l'inférence causale ne fournit pas une meilleure estimation, mais indique *la quantité d'informations que le design de recherche fournit pour cette assertion causale*.

- L'approche par estimation fournit une meilleure estimation, mais ne vous dit pas ce que vous savez sur cette estimation.
   - Par exemple, une estimation avec $N=10$ semble en dire moins sur l'effet que $N=1000$.
   - Par exemple, une estimation avec 95% de $Y=1$ et 5% de $Y=0$ semble en dire moins que lorsque les résultats sont répartis également entre 0 et 1.

- Nous rapportons presque toujours les deux approches, car les deux nous aident à prendre des décisions : "Notre estimation de l'effet du traitement était de 5, et nous pouvions rejeter l'idée que l'effet était de 0 ($p$=.01)."


# Les basics du test d'hypothèse

## Ingrédients d'un test d'hypothèse

 - Une **hypothèse** est un énoncé concernant une relation entre les résultats potentiels.

- Une **statistique de test** résume la relation entre le traitement et les résultats observés.

- Le **design** permet de lier l'hypothèse et la statistique de test : calculez une statistique de test qui décrit une relation entre des résultats potentiels.

- Le **design** indique aussi comment générer une *distribution* des statistiques de test possibles implicitement liés à l'hypothèse.

- Une **$p$-valeur** décrit la relation entre notre statistique de test observée et la distribution des statistiques de test hypothétiques.

```{r echo=FALSE}
## Tout d'abord, créer des données
##  y0 est le résultat potentiel à contrôler
N <- 10
y0 <- c(0, 0, 0, 1, 1, 3, 4, 5, 190, 200)
## Différents effets de traitement au niveau individuel
tau <- c(10, 30, 200, 90, 10, 20, 30, 40, 90, 20)
## y1 est le résultat potentiel du traitement
y1 <- y0 + tau
# sd(y0)
# mean(y1)-mean(y0)
# mean(tau)
## T est l'assignation du traitement
set.seed(12345)
T <- complete_ra(N)
## Y est les résultats observés
Y <- T * y1 + (1 - T) * y0
## Les données
dat <- data.frame(Y = Y, T = T, y0 = y0, tau = tau, y1 = y1)
dat$Ybin <- as.numeric(dat$Y > 100)
# dat
# pvalue(oneway_test(Y~factor(T),data=dat,distribution=exact(),alternative="less"))
# pvalue(wilcox_test(Y~factor(T),data=dat,distribution=exact(),alternative="less"))
```


```{r echo=FALSE}
## créer un dataset plus grand
##  y0 est le résultat potentiel à contrôler
bigN <- 60
set.seed(12345)
bigdat <- data.frame(y0 = c(rep(0, 20), rnorm(20, mean = 3, sd = .5), rnorm(20, mean = 150, sd = 10)))
## Différents effets de traitement au niveau individuel
bigdat$tau <- c(rnorm(20, mean = 10, sd = 2), rnorm(20, mean = 20, sd = 5), rnorm(20, mean = 5, sd = 10))
## y1 est le résultat potentiel du traitement
bigdat$y1 <- bigdat$y0 + bigdat$tau
# sd(y0)
# mean(y1)-mean(y0)
# mean(tau)
## T est l'assignation du traitement
set.seed(12345)
bigdat$T <- complete_ra(bigN)
## Y est les résultats observés
bigdat$Y <- with(bigdat, T * y1 + (1 - T) * y0)
## Les données
bigdat$Ybin <- as.numeric(bigdat$Y > quantile(bigdat$Y, .85))
```

## Une hypothèse est l'énoncé ou le modèle d'une relation entre des résultats potentiels

```{r}
kableExtra::kable(dat, col.names = c("Outcome", "Treatment", "$y_{i,0}$", "ITE", "$y_{i,1}$", "$Y>0$"), escape = FALSE)
```

Par exemple, l'hypothèse nulle stricte d'absence d'effet est $H_0 : y_{i,1} = y_{i,0}$


## Les statistiques des test résument les relations entre le traitement et les résultats

```{r, echo=TRUE}
## La statistique du test des différences moyennes
meanTT <- function(ys, z) {
  mean(ys[z == 1]) - mean(ys[z == 0])
}
## La statistique du test de la différence des moyennes selon le rang
meanrankTT <- function(ys, z) {
  ranky <- rank(ys)
  mean(ranky[z == 1]) - mean(ranky[z == 0])
}

observedMeanTT <- meanTT(ys = Y, z = T)
observedMeanRankTT <- meanrankTT(ys = Y, z = T)
observedMeanTT
observedMeanRankTT
```

## Le design lie la statistique de test et l'hypothèse

Ce que nous observons pour chaque personne $i$ ($Y_i$) est soit ce que nous aurions observé en traitement ($y_{i,1}$) **ou** ce que nous aurions observé en contrôle ($y_{i ,0}$).

$$Y_i = T_i y_{i,1} + (1-T_i)* y_{i,0}$$

Donc, si $y_{i,1}=y_{i,0}$ alors $Y_i = y_{i,0}$.

Ce que nous *observons réellement* est ce que nous *aurions observé dans la condition de contrôle*.

## Le design guide la création d'une distribution de statistiques de tests hypothétiques

Il faut savoir comment répéter notre expérience:

```{r, echo=TRUE}
repeatExperiment <- function(N) {
  complete_ra(N)
}
```

Ensuite, on répèter notre expérience en calculant à chaque fois une nouvelle statistique de test donnée par l'hypothèse et la design :

```{r reps, echo=TRUE, cache=TRUE}
set.seed(123456)
possibleMeanDiffsH0 <- replicate(
  10000,
  meanTT(ys = Y, z = repeatExperiment(N = 10))
)
set.seed(123456)
possibleMeanRankDiffsH0 <- replicate(
  10000,
  meanrankTT(ys = Y, z = repeatExperiment(N = 10))
)
```

## Courbe des distributions de randomisation sous l'hypothèse nulle

```{r fig.cap="Utiliser un design d'expérience pour tester une hypothèse avec deux statistiques de test différentes.", results='asis', echo=FALSE, fig.align='center'}
par(mfrow = c(1, 2), mgp = c(2, .5, 0), mar = c(3, 3, 0, 0), oma = c(0, 0, 3, 0))
plot(density(possibleMeanDiffsH0),
  ylim = c(0, .04),
  xlim = range(possibleMeanDiffsH0),
  lwd = 2,
  main = "", # Différence moyenne des statistiques de test",
  xlab = "Différence moyenne consistante avec H0",
  cex.lab = 1.25, cex.axis = 1
)
rug(possibleMeanDiffsH0)
rug(observedMeanTT, lwd = 3, ticksize = .51)
text(observedMeanTT - 4, .022, "Statistique de test observée")

plot(density(possibleMeanRankDiffsH0),
  lwd = 2,
  ylim = c(0, .45),
  xlim = c(-10, 10), # range(possibleMeanDiffsH0),
  main = "", # Différence moyenne des statistiques de test avec rang",
  xlab = "Différence moyenne des rangs consistante avec H0",
  cex.lab = 1.25, cex.axis = 1
)
rug(possibleMeanRankDiffsH0)
rug(observedMeanRankTT, lwd = 3, ticksize = .9)
text(observedMeanRankTT, .45, "Statistique de test observée")

mtext(
  side = 3, outer = TRUE, cex = 1.75,
  text = expression(paste("Distributions de statistiques de test consistentes avec le design et ", H0:y[i1] == y[i0]))
)
```

## Les $p$-valeurs résument les graphiques

Comment devrions-nous interpréter ces $p$-valeurs unilatérales ?

```{r calcpvalues, echo=TRUE}
pMeanTT <- mean(possibleMeanDiffsH0 >= observedMeanTT)
pMeanRankTT <- mean(possibleMeanRankDiffsH0 >= observedMeanRankTT)
pMeanTT
pMeanRankTT
```

## Comment faire cela en R : COIN

```{r coinexample, echo=TRUE}
## avec le package coin
library(coin)
set.seed(12345)
pMean2 <- coin::pvalue(oneway_test(Y ~ factor(T),
  data = dat,
  distribution = approximate(nresample = 1000), alternative = "less"
))
dat$rankY <- rank(dat$Y)
pMeanRank2 <- coin::pvalue(oneway_test(rankY ~ factor(T),
  data = dat,
  distribution = approximate(nresample = 1000), alternative = "less"
))
pMean2
pMeanRank2
```

## Comment faire cela en R : RItools  {.allowframebreaks}

Installez d'abord une version de développement du package RItools

```{r installritools, eval=FALSE, echo=TRUE, results='hide',warnings=FALSE,cache=FALSE}
# dev_mode() ## dont install the package globally
renv::install("markmfredrickson/RItools@randomization-distribution",
  force = TRUE
)
# dev_mode()
```

Utilisez ensuite la fonction `RItest`.

```{r useritools, eval=FALSE,echo=TRUE,cache=FALSE}
# dev_mode()
library(RItools)
thedesignA <- simpleRandomSampler(total = N, z = dat$T, b = rep(1, N))
pMean4 <- RItest(
  y = dat$Y, z = dat$T, samples = 1000, test.stat = meanTT,
  sampler = thedesignA
)
pMeanRank4 <- RItest(
  y = dat$Y, z = dat$T, samples = 1000, test.stat = meanrankTT,
  sampler = thedesignA
)
pMean4
pMeanRank4
# dev_mode() ## et désactiver le dev_mode
```

```{r ritoolsoutput, echo=TRUE, eval=FALSE, tidy=FALSE}
pMean4
Call:  RItest(y = dat$Y, z = dat$T, test.stat = meanTT, sampler = thedesignA,
          samples = 1000)

                             Value Pr(>x)
Statistique de test observée -49.6   0.78

pMeanRank4
Call:  RItest(y = dat$Y, z = dat$T, test.stat = meanrankTT, sampler = thedesignA,
          samples = 1000)

                              Value Pr(>x)
Statistique de test observée  1   0.32
```

## Comment faire cela en R : RI2

Comment interpréter la $p$-valeur bilatérale ici ?

```{r,echo=TRUE}
## using the ri2 package
library(ri2)
thedesign <- declare_ra(N = N)
dat$Z <- dat$T
pMean4 <- conduct_ri(Y ~ Z,
  declaration = thedesign,
  sharp_hypothesis = 0, data = dat, sims = 1000
)
summary(pMean4)
pMeanRank4 <- conduct_ri(rankY ~ Z,
  declaration = thedesign,
  sharp_hypothesis = 0, data = dat, sims = 1000
)
summary(pMeanRank4)
```

## Sujets suivants

  - Tester l'hypothèse nulle faible, $H_0 : \bar{y}_{1} = \bar{y}_{0}$.

  - Rejeter l'hypothèse nulle (et faire des erreurs de faux positifs et/ou de faux négatifs).

  - Conserver un taux d'erreur correct de faux positifs quand on teste plus d'une hypothèse.

  - Puissance statistique des tests d'hypothèses ([Module sur la puissance statistique et les diagnosandes de design](https://egap.github.io/learningdays-book/statistical-power-and-design-diagnosands.html)).

## Tester l'hypothèse nulle faible d'absence d'effet de traitement moyen

- L'hypothèse nulle faible est une affirmation sur les agrégats, et elle est presque toujours exprimée en termes de moyennes : $H_0 : \bar{y}_{1} = \bar{y}_{0}$

- La statistique de test pour cette hypothèse est presque toujours la simple différence des moyennes (c'est-à-dire `meanTT()` ci-dessus).

```{r simpdiffs, echo=TRUE}
lm1 <- lm(Y ~ T, data = dat)
lm1P <- summary(lm1)$coef["T", "Pr(>|t|)"]
ttestP1 <- t.test(Y ~ T, data = dat)$p.value
library(estimatr)
ttestP2 <- difference_in_means(Y ~ T, data = dat)
c(lm1P = lm1P, ttestP1 = ttestP1, tttestP2 = ttestP2$p.value)
```

- Pourquoi la $p$-valeur pour les moindres carrés est-elle différente ? Quelles hypothèses utilisons-nous pour la calculer ?

## Tester l'hypothèse nulle faible d'absence d'effet de traitement moyen

La variation et l'emplacement de $Y$ changent avec le traitement dans cette simulation.

```{r fig.cap="Résultats observés en fonction du statut de traitement", results='asis', out.width=".7\\textwidth"}
boxplot(Y ~ T, données = dat)
```

## Tester l'hypothèse nulle faible d'absence d'effet de traitement moyen

```{r, echo=TRUE}
## à la main:
varEstATE <- function(Y, T) {
  var(Y[T == 1]) / sum(T) + var(Y[T == 0]) / sum(1 - T)
}
seEstATE <- sqrt(varEstATE(dat$Y, dat$T))
obsTStat <- observedMeanTT / seEstATE
c(
  observedTestStat = observedMeanTT,
  stderror = seEstATE,
  tstat = obsTStat,
  pval = 2 * min(
    pt(obsTStat, df = 8, lower.tail = TRUE),
    pt(obsTStat, df = 8, lower.tail = FALSE)
  )
)
```

# Rejeter l'hypothèse nulle

## Rejeter l'hypothèse nulle et faire des erreurs

- "Typiquement, le niveau du test [$\alpha$] est une promesse sur les performances du test et la taille est un fait sur ses performances..." (Rosenbaum 2010, Glossaire)

- $\alpha$ est la probabilité de rejeter l'hypothèse nulle lorsque l'hypothèse nulle est vraie.

- Comment doit-on interpréter $p$=`r round(pMeanTT,2)` ? Qu'en est-il de $p$=`r round(pMeanRankTT,2)` (nos tests pour l'hypothèse nulle stricte) ?

- Que signifie "rejeter" $H_0 : y_{i,1}=y_{i,2}$ à $\alpha=.05$ ?


## Taux de faux positifs dans les tests d'hypothèses {.allowframebreaks}

```{r normp, echo=FALSE,out.width=".5\\textwidth",fig.cap="P-valeur unilatérale d'une statistique de test normalement."}
library(tidyverse)
ggplot(NULL, aes(c(-3, 3))) +
  geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(2, 3)) +
  geom_area(stat = "function", fun = dnorm, fill = "grey80", xlim = c(-3, 2)) +
  labs(x = "test stat (center=0)", y = "prob") +
  geom_vline(xintercept = 2) +
  scale_y_continuous(breaks = NULL) +
  # scale_x_continuous(breaks = 4) +
  theme_classic()
```

Notice:

 - The curve is centered at the hypothesized value.
 
 - The curve represents the world of the hypothesis.
 
 - The $p$-value is how rare it would be to see the observed test statistic (or a value farther away from the hypothesized value) in the world of the null.
 
 - In the picture, the observed value of the test statistic is consistent with the hypothesized distribution, but just not super consistent.
 
 - Even if $p < .05$ (or $p < .001$) the observed test statistic must reflect some value on the hypothesized distribution. This means that you can always make an error when you reject a null hypothesis.

Attention:

  - La courbe est centrée à la valeur hypothétique.

  - La courbe représente le monde de l'hypothèse.

  - La $p$-valeur décrit à quel point il serait rare de voir la statistique de test observée (ou une valeur plus éloignée de la valeur hypothétique) dans le monde de l'hypothèse nulle.

  - Sur la figure, la valeur observée de la statistique de test est cohérente avec la distribution hypothétique, mais pas très cohérente.

  - Même si $p < .05$ (ou $p < .001$) la statistique de test observée doit refléter en partie la distribution hypothétique. Cela signifie que vous pouvez toujours faire une erreur lorsque vous rejetez une hypothèse nulle.

## False positive and false negative errors 

- If we say, "The experimental result is significantly different from the hypothesized value of zero ($p=.001$) ! We reject that hypothesis !" **when the truth is zero** we are making a **false positive error** (claiming to detect something positively when there is no signal, only noise).

- If we say, "We cannot distinguish this result from zero ($p=.3$). We cannot reject the hypothesis of zero." **when the truth is not zero** we are making a **false negative error** (claiming inability to detect something when there is a signal, but it is overwhelmed by noise.)

## Faux positifs et faux négatifs

- Si nous disons, "le résultat expérimental est significativement différent de la valeur hypothétique de zéro ($p=.001$) ! Nous rejetons cette hypothèse !"
  **lorsque la vérité est zéro** nous faisons une **erreur de faux positifs** (prétendant détecter quelque chose lorsqu'il n'y a pas de signal, seulement du bruit).

- Si nous disons : "Nous ne pouvons pas distinguer ce résultat de zéro ($p=.3$). Nous ne pouvons pas rejeter l'hypothèse de zéro."
  **lorsque la vérité n'est pas zéro** nous faisons une **erreur de faux négatifs** (prétendant incapacité à détecter quelque chose lorsqu'il y a un signal, mais qu'il est submergé par le bruit.)

## A single test of a single hypothesis

- A single test of a single hypothesis should encourage false positive errors rarely (for example, if we set $\alpha=.05$) then we are saying that we are comfortable with our testing procedure making false positive errors in **no more than 5% of tests of a given treatment assignment in a given experiment**.

- Also, a **single test of a single hypothesis** should detect signal when it exists --- it should be have high **statistical power**. In other words, it should not fail to detect a signal when it exists (i.e. should have low false negative error rates).

## Un test unique d'une seule hypothèse

- Un test unique d'une seule hypothèse devrait rarement augmenter le taux de faux positifs
  (par exemple, si nous définissons $\alpha=.05$) alors nous acceptons que notre procédure de test produise des faux positifs
  dans **au plus de 5% des tests d'une assignation de traitement donnée dans une expérience donnée**.

- De plus, un **test unique d'une seule hypothèse** doit détecter le signal lorsqu'il existe --- il doit avoir une **puissance statistique** élevée.
  En d'autres termes, il ne doit pas manquer la détection du signal lorsqu'il existe (c'est-à-dire qu'il devrait avoir un faible taux de faux négatifs).

## Les décisions impliquent des erreurs

- Si les erreurs sont nécessaires, comment les diagnostiquer ? Comment savoir si notre procédure de tests d'hypothèses génère trop de faux positifs ?

- Diagnostiquez par simulation !

## Diagnostiquer les taux de faux positifs par simulation

- A travers les répétitions du design :

   - Créer une hypothèse nulle vraie.
   - Testez cette hypothèse.
   - La $p$-valeur doit être élevée si le test fonctionne correctement.

- La proportion de petites valeurs $p$ ne doit pas dépasser $\alpha$ si le test fonctionne correctement.

## Diagnostiquer les taux de faux positifs par simulation

Exemple avec un résultat binaire. Le test fonctionne-t-il comme il se doit ? À quoi ressemblent les p-valeurs lorsqu'il n'y a pas d'effet ?

```{r, echo=TRUE}
collectPValues <- function(y, trt, thedistribution = exact()) {
  ## Faire en sorte que Y et T n'aient aucune relation en randomisant T à nouveau
  new_trt <- repeatExperiment(length(y))
  thedata <- data.frame(new_trt = new_trt, y = y)
  thedata$ranky <- rank(y)
  thedata$new_trtF <- factor(thedata$new_trt)
  ## Les 4 tests
  thelm <- lm(y ~ new_trt, data = thedata)
  t_test_CLT <- difference_in_means(y ~ new_trt, data = thedata)
  t_test_exact <- oneway_test(y ~ new_trtF,
    data = thedata,
    distribution = thedistribution
  )
  t_test_rank_exact <- oneway_test(ranky ~ new_trtF,
    data = thedata,
    distribution = thedistribution
  )
  owP <- coin::pvalue(t_test_exact)[[1]]
  owRankP <- coin::pvalue(t_test_rank_exact)[[1]]
  ## Renvoyer les p-valeurs
  return(c(
    lmp = summary(thelm)$coef["new_trt", "Pr(>|t|)"],
    neyp = t_test_CLT$p.value[[1]],
    rtp = owP,
    rtpRank = owRankP
  ))
}
```

```{r fprdsim, cache=TRUE, message=FALSE, warning=FALSE, results='hide'}
set.seed(123456)
theY <- dat$Ybin
theT <- dat$T
pDist <- replicate(5000, collectPValues(y = theY, trt = theT))
```

## Diagnostiquer les taux de faux positifs par simulation

- Lorsqu'il n'y a pas d'effet, un test de l'hypothèse nulle d'absence d'effet de traitement doit produire une **grande** p-valeur.

- Si le test fonctionne bien, nous devrions voir principalement de grandes p-valeurs et très peu de petites p-valeurs.

- Quelques-unes des p-valeurs pour les quatre tests différents (nous avons fait 5000 simulations, en montrant juste 5)

```{r, echo=FALSE}
pDist[, 1:5]
```

## Diagnostiquer les taux de faux positifs par simulation

En fait, s'il n'y a pas d'effet, et si nous décidons de rejeter l'hypothèse nulle d'absence d'effet de traitement avec $\alpha=.25$, nous ne voudrions **pas plus de 25% des p-valeurs de cette simulation en dessous de p=.25**.
Que voit-on ici ? Quels tests semblent avoir des taux de faux positifs trop élevés ?

```{r pdistsummary, echo=TRUE}
## Calculer la proportion de p-valeurs inférieures à 0,25 pour chaque ligne de pDist
apply(pDist, 1, function(x) {
  mean(x < .25)
})
```

## Diagnostiquer les taux de faux positifs par simulation

Comparez les tests en traçant la proportion de p-valeurs inférieures à un nombre donné.
Les tests "d'inférence de randomisation" contrôlent le taux de faux positifs
(ce sont les tests avec permutation directe répétant l'expérience).

```{r plotecdf, results='asis', echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distributions des p-valeurs quand il n y a aucun effet pour quatre tests avec n=10. Un test qui contrôle son taux de faux positifs doit avoir des points sur ou en dessous de la ligne diagonale", out.width='.6\\textwidth'}
par(mfrow = c(1, 1), mgp = c(2, .75, 0), oma = rep(0, 4), mar = c(3.5, 3.5, 0, 0))
plot(c(0, 1), c(0, 1),
  type = "n",
  xlab = "p-valeur = p", ylab = "Proportion des p-valeurs < p",
  cex.lab = 2, cex.axis = 2
)
for (i in 1:nrow(pDist)) {
  lines(ecdf(pDist[i, ]), pch = i, col = i, cex = 2, cex.axis = 2, lwd = 2)
}
abline(0, 1, col = "gray")
legend("topleft",
  legend = c("Moindre carrés", "Neyman", "Diff des moyennes d'inférence de randomisation", "Diff des moyennes d'inférence de randomisation selon le rang"),
  pch = 1:5, col = 1:5, lty = 1, bty = "n", cex = 2
)
```

## Taux de faux positifs avec $N=60$ et résultat binaire

Dans ce design, seuls les tests basés sur l'inférence de randomisation directe contrôlent le taux de faux positifs.

```{r fprdsimBig, cache=TRUE}
set.seed(12345)
## pDistBig <- replicate(1000,collectPValues(y=bigdat$Ybin,z=bigdat$T,thedistribution=approximate(B=1000)))
library(parallel)
pDistBigLst <- mclapply(1:1000, function(i) {
  collectPValues(y = bigdat$Ybin, trt = bigdat$T, thedistribution = approximate(nresample = 1000))
}, mc.cores = 8)
pDistBig <- simplify2array(pDistBigLst)
```

```{r plotecdfBig, results='asis', echo=FALSE, message=FALSE, warning=FALSE,out.width='.7\\textwidth',fig.cap="Distributions des p-valeurs quand il n y a aucun effet pour quatre tests avec n=60 et un résultat binaire. Un test qui contrôle son taux de faux positifs doit avoir des points sur ou en dessous de la ligne diagonale."}
par(mfrow = c(1, 1), mgp = c(2, .75, 0), oma = rep(0, 4), mar = c(3.5, 3.5, 0, 0))
plot(c(0, 1), c(0, 1),
  type = "n",
  xlab = "p-valeur = p", ylab = "Proportion des p-valeurs < p",
  cex.lab = 2, cex.axis = 2
)
for (i in 1:nrow(pDistBig)) {
  lines(ecdf(pDistBig[i, ]), pch = i, col = i, cex = 2, cex.axis = 2, lwd = 2)
}
abline(0, 1, col = "gray")
legend("topleft",
  legend = c("Moindre carrés", "Neyman", "Diff des moyennes d'inférence de randomisation", "Diff des moyennes d'inférence de randomisation selon le rang"),
  pch = 1:5, col = 1:5, lty = 1, bty = "n", cex = 2
)
```

## Taux de faux positifs avec $N=60$ et résultat continu

Ici, tous les tests contrôlent bien le taux de faux positifs.

```{r fprdsimBig2, cache=TRUE}
set.seed(123456)
pDistBigLst2 <- mclapply(1:1000, function(i) {
  collectPValues(y = bigdat$Y, trt = bigdat$T, thedistribution = approximate(nresample = 1000))
}, mc.cores = 8)
pDistBig2 <- simplify2array(pDistBigLst2)
```

```{r plotecdfBig2, results='asis', echo=FALSE, message=FALSE, warning=FALSE, out.width='.7\\textwidth',fig.cap='Distributions des p-valeurs quand il n y a aucun effet pour quatre tests avec n=60 et un résultat continu. Un test qui contrôle son taux de faux positifs doit avoir des points sur ou en dessous de la ligne diagonale.'}
library(scales)
par(mfrow = c(1, 1), mgp = c(2, .75, 0), oma = rep(0, 4), mar = c(3.5, 3.5, 0, 0))
plot(c(0, 1), c(0, 1),
  type = "n",
  xlab = "p-valeur = p", ylab = "Proportion des p-valeurs < p",
  cex.lab = 2, cex.axis = 2
)
for (i in 1:nrow(pDistBig2)) {
  lines(ecdf(pDistBig2[i, ]), pch = i, col = alpha(i, .5), cex = 2, cex.axis = 2)
}
abline(0, 1, col = "gray")
legend("topleft",
  legend = c("Moindre carrés", "Neyman", "Diff des moyennes d'inférence de randomisation", "Diff des moyennes d'inférence de randomisation selon le rang"),
  pch = 1:5, col = 1:5, lty = 1, bty = "n", cex = 2
)
```

## Sommaire

- Un bon test :

     1. met rarement en doute la vérité, et

     2. distingue facilement le signal du bruit (met souvent en doute les contrevérités).

- Nous pouvons savoir si notre procédure de test contrôle le taux de faux positifs compte tenu de notre dsign.

- Lorsque le taux de faux positifs n'est pas contrôlé, qu'est-ce qui ne va pas ? (probablement l'asymptotique.)

# Approfondir

## Approfondir les tests d'hypothèses

  - Même si une procédure de test donnée contrôle le taux de faux positifs pour un seul test, elle peut ne pas contrôler le taux pour un groupe de tests.
    Voir [10 choses à savoir sur les comparaisons multiples](https://egap.org/resource/10-things-to-know-about-multiple-comparisons/) pour les approches de contrôle de taux de rejet dans plusieurs tests.

  - Un intervalle de confiance de 100(1-\alpha)$\% peut être défini comme la plage d'hypothèses où toutes les $p$-valeurs sont supérieures ou égales à $\alpha$.
    C'est ce qu'on appelle l'inversion du test d'hypothèse (@rosenbaum2010design). Autrement dit, un intervalle de confiance est un ensemble de tests d'hypothèses.

## Que savoir de plus sur les tests d'hypothèses {.allowframebreaks}

  - Une estimation de position basée sur un test d'hypothèse est appelée une estimation de position de Hodges-Lehmann (@rosenbaum1993hlp,@hodges1963elb).

  - Un ensemble de tests d'hypothèses peut être combiné en un seul test d'hypothèse (@hansen:bowers:2008,@caughey2017nonparametric).

  - TODO Pour tester l'équivalence, on peut supposer que deux statistiques de test sont équivalentes (c'est-à-dire que le groupe de traitement est le même que le groupe de contrôle) plutôt qu'une seule (la différence entre les deux groupes est nulle) (@hartman2018equivalence).

  - Étant donné qu'un test d'hypothèse est un modèle de résultats potentiels, on peut utiliser les tests d'hypothèses pour en savoir plus sur des modèles complexes, tels que des modèles de débordement et de propagation des effets de traitement à travers les réseaux (@bowers2013reasoning, @bowers2016research, @bowers2018models)

## Exercice : Tests d'hypothèses et statistiques de test

  1. Si une intervention était très efficace pour augmenter la variabilité d'un résultat mais ne changeait pas la moyenne, la $p$-valeur rapportée par R ou Stata en utilisant `lm_robust()` ou `difference_of_means()` ou ` reg` ou `t.test` est-elle grande ou petite ?

  2. Si une intervention réduisait modérément la moyenne dans le groupe témoin mais augmentait considérablement quelques résultats (comme un effet 10 fois supérieur), la $p$-valeur de R `lm_robust()` ou `difference_of_means() ` est-elle grande ou petite ?

# Tester de nombreuses hypothèses

## Quand pouvons-nous tester de nombreuses hypothèses ?

- L'effet d'un traitement expérimental diffère-t-il entre les différents groupes ? Les différences de l'effet du traitement pourraient-elles survenir en raison de certaines caractéristiques de base des sujets expérimentaux ?

- Parmi plusieurs stratégies de communication, lesquelles ont été les plus efficaces sur un résultat particulier ?

- Parmi plusieurs résultats, lesquels ont été influencés par une seule intervention expérimentale ?

## Taux de faux positifs dans les tests d'hypothèses multiples {.fragile}

Disons que notre probabilité de faux positifs est de 0,05 pour un seul test. Que se passe-t-il si nous demandons:
(1) lequel de ces 10 résultats a une relation statistiquement significative avec les deux bras de traitement ?
(2) *lequel de ces 10 bras de traitement avait une relation statistiquement significative avec ce résultat particulier* ?

  - La probabilité de faux positifs doit être inférieure ou égale à 0,05 dans un test.
  - La probabilité de faux positifs doit être inférieure ou égale à $1 - ( ( 1 - .05 ) \times (1 - .05) ) = .0975$ dans 2 tests.
  - La probabilité d'avoir au moins un faux positif avec $\alpha=.05$ dans 10 tests devrait être $\le$ $1 - (1-.05)^{10}=.40$.

## Discoveries with multiple tests

**Number of errors committed when testing $m$ null hypotheses** [@benjamini1995's Table 1]. Cells are numbers of tests.  $R$ is # of "discoveries" and $V$ is # of false discoveries, $U$ is # of correct non-rejections, and $S$ is # of correct rejections.

**Nombre d'erreurs commises en testant $m$ hypothèses nulles** [Table 1 de @benjamini1995].
Les cellules sont le nombre de tests. $R$ est le nombre de "découvertes" et $V$ est le nombre de fausses découvertes,
$U$ est le nombre de non-rejets corrects et $S$ est le nombre de rejets corrects.

+------------------------------------------+--------------------------+----------------------+-----------+
|                                          | Declarés                 | Declarés             |   Total   |
|                                          | Non-Significant          | Significant          |           |
+==========================---=============+:========================:+:====================:+:=========:+
| hypothèse nulle vraie ($H_{true}=0$)     |             U            |           V          |   $m_0$   |
+------------------------------------------+--------------------------+----------------------+-----------+
| hypothèse nulle fause ($H_{true} \ne 0$) |             T            |           S          | m - $m_0$ |
+------------------------------------------+--------------------------+----------------------+-----------+
| Total                                    |            m-R           |           R          |    m      |
+------------------------------------------+--------------------------+----------------------+-----------+

## Deux taux d'erreur principaux à contrôler lors du test de nombreuses hypothèses {.allowframebreaks}

1. **Le taux d'erreur par famille (family wise error rate / FWER)** est de $P(V>0)$ (Probabilité de tous les faux positifs).

    - à contrôler si nous prévoyons de prendre une décision sur les résultats de nos multiples tests. Le projet de recherche est essentiellement confirmatoire.

    - voir par exemple les projets de l'OES <https://oes.gsa.gov> : les agences fédérales choisissent quand elles détectent des résultats.

2. **Le taux d'erreurs de type I pour l'hypothèse nulle (false discovery rate / FDR)** est de $E(V/R | R>0)$ (proportion moyenne de faux positifs compte tenu de certains rejets).

    - à contrôler si nous utilisons *cette* expérience pour planifier *la prochaine* expérience. Nous sommes prêts à accepter une probabilité d'erreur plus élevée dans l'intérêt de nous donner plus de possibilités de découverte.

    - par exemple, on pourrait imaginer une organisation, un gouvernement ou une ONG qui déciderait de mener *une série* d'expériences dans le cadre d'un *programme d'apprentissage* : aucune expérience ne détermine la prise de décision, plus de place pour l'exploration.

Nous nous concentrerons sur le FWER mais recommandons de penser au FDR pour les programmes d'apprentissage.

## Questions à résultats multiples

- Quel est l'effet d'un traitement sur plusieurs résultats ?

- Sur quels résultats (parmi beaucoup) le traitement a-t-il eu un effet ?

- La deuxième question, en particulier, peut conduire à problèmes de taux d'erreur par famille (voir ci-dessus).

## Tests d'hypothèses multiples : résultats multiples

Imaginez que nous ayons cinq résultats et un traitement (montrant ici les résultats potentiels et observés) :

```{r multtesting1}
set.seed(23)
thedat <- fabricate(
  N = 100,
  y0_1 = rnorm(N),
  y0_2 = rnorm(N),
  y0_3 = rnorm(N),
  y0_4 = rnorm(N),
  y0_5 = rnorm(N)
)
tau1 <- 0
tau5 <- tau4 <- tau3 <- tau2 <- tau1
thepop <- declare_population(thedat)
theassign <- declare_assignment(T = complete_ra(N = N, m = 50))
po_1 <- declare_potential_outcomes(Y1_T_0 = y0_1, Y1_T_1 = y0_1 + tau1)
po_2 <- declare_potential_outcomes(Y2_T_0 = y0_2, Y2_T_1 = y0_2 + tau2)
po_3 <- declare_potential_outcomes(Y3_T_0 = y0_3, Y3_T_1 = y0_3 + tau3)
po_4 <- declare_potential_outcomes(Y4_T_0 = y0_4, Y4_T_1 = y0_4 + tau4)
po_5 <- declare_potential_outcomes(Y5_T_0 = y0_5, Y5_T_1 = y0_5 + tau5)
reveal_1 <- declare_reveal(Y1, T)
reveal_2 <- declare_reveal(Y2, T)
reveal_3 <- declare_reveal(Y3, T)
reveal_4 <- declare_reveal(Y4, T)
reveal_5 <- declare_reveal(Y5, T)

des1 <- thepop + theassign +
  po_1 + po_2 + po_3 + po_4 + po_5 +
  reveal_1 + reveal_2 + reveal_3 + reveal_4 + reveal_5

dat1 <- draw_data(des1)
options(digits = 2)
head(dat1[, -c(2:6, 18:22)])
head(dat1[, c(1, 7, 18:22)])
```

## Pouvons-nous détecter un effet sur le résultat `Y1` ?

Pouvons-nous détecter un effet sur le résultat `Y1` ? (c'est-à-dire, le test d'hypothèse produit-il une $p$-valeur suffisamment petite ?)
```{r p1, echo=TRUE}
coin::pvalue(oneway_test(Y1 ~ factor(T), data = dat1))
## Notez que la p-valeur du test t est également un test du chi carré
coin::pvalue(independence_test(Y1 ~ factor(T),
  data = dat1,
  teststat = "quadratic"
))
```

## Pour lequel des cinq résultats pouvons-nous détecter un effet ?

Pouvons-nous détecter un effet sur le résultat `Y1` ? (c'est-à-dire, l'un des cinq tests d'hypothèses produit-il une $p$-valeur suffisamment petite ?)

```{r pmult, echo=TRUE}
p1 <- coin::pvalue(oneway_test(Y1 ~ factor(T), data = dat1))
p2 <- coin::pvalue(oneway_test(Y2 ~ factor(T), data = dat1))
p3 <- coin::pvalue(oneway_test(Y3 ~ factor(T), data = dat1))
p4 <- coin::pvalue(oneway_test(Y4 ~ factor(T), data = dat1))
p5 <- coin::pvalue(oneway_test(Y5 ~ factor(T), data = dat1))
theps <- c(p1 = p1, p2 = p2, p3 = p3, p4 = p4, p5 = p5)
sort(theps)
```

## Pouvons-nous détecter un effet pour tous les résultats ?

Pouvons-nous détecter un effet pour tous les résultats ? (c'est-à-dire, un test d'hypothèse pour tous les résultats ensemble produit-il une $p$-valeur suffisamment petite ?)
```{r omnibus, echo=TRUE}
coin::pvalue(independence_test(Y1 + Y2 + Y3 + Y4 + Y5 ~ factor(T),
  data = dat1, teststat = "quadratic"
))
```

Quelle approche est susceptible de nous induire en erreur avec trop de résultats "statistiquement significatifs" (5 tests ou 1 test omnibus) ?

## Comparer les approches I

Faisons une simulation pour en savoir plus sur ces approches de test.

- Nous allons (1) définir les véritables effets causals à 0, (2) réassigner le traitement à plusieurs reprises, et (3) à chaque fois, faire chacun de ces trois tests.

- Puisque le véritable effet est 0, nous nous attendons à ce que *la plupart* des $p$-values soient grandes.
(En fait, nous ne voulons pas plus de 5% des $p$-values supérieures à $p=.05$ si nous utilisons le critère d'acceptance ou de rejet $\alpha=.05$).

```{r testsetup1, echo=FALSE, results="hide"}
ttest_Y1fn <- function(data) {
  p1 <- coin::pvalue(oneway_test(Y1 ~ factor(T), data = data))
  return(data.frame(statistic = NA, p.value = p1))
}
ttest_multfn <- function(data) {
  p1 <- coin::pvalue(oneway_test(Y1 ~ factor(T), data = data))
  p2 <- coin::pvalue(oneway_test(Y2 ~ factor(T), data = data))
  p3 <- coin::pvalue(oneway_test(Y3 ~ factor(T), data = data))
  p4 <- coin::pvalue(oneway_test(Y4 ~ factor(T), data = data))
  p5 <- coin::pvalue(oneway_test(Y5 ~ factor(T), data = data))
  theps <- c(p1, p2, p3, p4, p5)
  return(data.frame(statistic = NA, p.value = min(theps)))
}
ttest_mult_holmfn <- function(data) {
  p1 <- coin::pvalue(oneway_test(Y1 ~ factor(T), data = data))
  p2 <- coin::pvalue(oneway_test(Y2 ~ factor(T), data = data))
  p3 <- coin::pvalue(oneway_test(Y3 ~ factor(T), data = data))
  p4 <- coin::pvalue(oneway_test(Y4 ~ factor(T), data = data))
  p5 <- coin::pvalue(oneway_test(Y5 ~ factor(T), data = data))
  theps <- c(p1, p2, p3, p4, p5)
  padj <- p.adjust(theps, method = "holm")
  minp <- min(padj)
  return(data.frame(statistic = NA, p.value = minp))
}
ttest_omnibusfn <- function(data) {
  thep <- coin::pvalue(independence_test(Y1 + Y2 + Y3 + Y4 + Y5 ~ factor(T), data = data, teststat = "quadratic"))
  return(data.frame(statistic = NA, p.value = thep))
}

ttest_Y1 <- declare_test(handler = label_test(ttest_Y1fn), label = "test-t pour Y1")
ttest_mult <- declare_test(handler = label_test(ttest_multfn), label = "test-t pour tous")
ttest_mult_holm <- declare_test(handler = label_test(ttest_mult_holmfn), label = "test-t pour tous holm adj")
ttest_omnibus <- declare_test(handler = label_test(ttest_omnibusfn), label = "test-t omnibus")
ttest_Y1(dat1)
ttest_mult(dat1)
ttest_mult_holm(dat1)
ttest_omnibus(dat1)
```


```{r des1setup,echo=FALSE}
des1_plus <- des1 + ttest_Y1 + ttest_mult + ttest_mult_holm + ttest_omnibus
thediagnosands <- declare_diagnosands(fwer = mean(p.value < .05))
### Erreurs lors de la recherche d'un estimateur
## des1_diag <- diagnose_design(design = des1_plus, bootstrap_sims = 0,
##     sims=10, diagnosands = thediagnosands)
```

```{r dd1, echo=TRUE, cache=TRUE}
des1_sim <- simulate_design(des1_plus, sims = 1000)
res1 <- des1_sim %>%
  group_by(estimator) %>%
  summarize(fwer = mean(p.value < .05), .groups = "drop")
```

## Comparer les approches II
```{r}
kableExtra::kable(res1, caption = "Taux d'erreur par famille")
```

- L'approche utilisant 5 tests produit un $p < .05$ beaucoup trop souvent --- rappelons qu'il n'y a aucun effet causal pour aucun de ces résultats.

  - Un test d'un seul résultat (ici `Y1`) a $p < .05$ pour moins de 5% des simulations.

  - Le test omnibus montre également un taux d'erreur bien maîtrisé.

  - L'utilisation d'une correction de tests multiples (ici nous utilisons la correction "Holm") contrôle également correctement le taux de faux positifs.

## La correction de Holm

Comment utiliser la correction Holm (notez ce qui arrive aux $p$-valeurs):

```{r holmex, echo=TRUE}
theps
p.adjust(theps, method = "holm")
## Pour montrer ce qui se passe avec des p-valeurs "significatives"
theps_new <- sort(c(theps, newlowp = .01))
p.adjust(theps_new, method = "holm")
```

## Tests d'hypothèses multiples : bras de traitement multiples {.allowframebreaks}

- Le même genre de problème peut arriver lorsque la question porte sur l'effet de traitement différentiel multi-bras.

- Avec 5 bras, "l'effet du bras 1" pourrait signifier beaucoup de choses différentes : "Est-ce que les résultats potentiels moyens sont plus grands pour le bras 1 ou 2 ?",
"Les résultats potentiels sont-ils plus grands pour le bras 1 comparé à tous les autres ?"

- Si nous nous concentrons uniquement sur les comparaisons par paires de bras, nous pourrions avoir $((5 \times 5) - 5)/2 = 10$ tests uniques !

## Tests d'hypothèses multiples : bras de traitement multiples {.allowframebreaks}

Voici quelques résultats potentiels et observés quand `T` prend plusieurs valeurs.

```{r multitreatsetup, echo=FALSE}
theassign_mult <- declare_assignment(T = conduct_ra(N = N, num_arms = 5, conditions = c("1", "2", "3", "4", "5")))
po_mult <- declare_potential_outcomes(Y ~ y0_1 * (T == "1") + y0_2 * (T == "2") +
  y0_3 * (T == "3") + y0_4 * (T == "4") + y0_5 * (T == "5"),
conditions = c("1", "2", "3", "4", "5"),
assignment_variables = T
)
reveal_mult <- declare_reveal(assignment_variables = T)
des2 <- thepop + theassign_mult + po_mult + reveal_mult
dat2 <- draw_data(des2)
options(digits = 2)
## T est le bras de traitement : 1,2,3,4,5
head(dat2[, -c(2:6, 8)])
options(digits = 4)
```

## Tests d'hypothèses multiples : bras de traitement multiples {.allowframebreaks}

Voici les 10 tests par paire avec et sans ajustement pour les tests multiples. Remarquez comment un résultat "significatif" ($p=.01$) change avec l'ajustement.

```{r}
## ceci est une interface à independence_test() dans coin
pair_tests1 <- pairwisePermutationTest(Y ~ T, data = dat2, distribution = asymptotic(), method = "holm", teststat = "quadratic")
pair_tests1
```
## Tests d'hypothèses multiples : bras de traitement multiples

Nous illustrons quatre approches différentes :

   1. faire tous les tests par paire et choisir le meilleur (une mauvaise idée) ;
   2. faire tous les tests par paire et choisir le meilleur après avoir ajusté les p-valeurs pour les tests multiples (une bonne idée mais avec une très faible puissance statistique) ;
   3. tester l'hypothèse d'absence de relation pour *tous les bras* (un test omnibus) et le résultat (une bonne idée) ;
   4. choisissez un bras sur lequel vous concentrer à l'avance (une bonne idée).

```{r}
ttest_T1_vs_allfn <- function(data) {
  p1 <- coin::pvalue(oneway_test(Y ~ factor(T == "1"), data = data))
  return(data.frame(statistic = NA, p.value = p1))
}
overall_Tfn <- function(data) {
  itest <- independence_test(Y ~ T, data = data)
  return(data.frame(statistic = NA, p.value = coin::pvalue(itest)))
}
pairwise_testsfn <- function(data) {
  pair_tests <- pairwisePermutationTest(Y ~ T, data = data, distribution = asymptotic(), method = "holm", teststat = "quadratic")
  return(data.frame(statistic = NA, p.value = min(pair_tests$p.value)))
}
pairwise_tests_adjfn <- function(data) {
  pair_tests <- pairwisePermutationTest(Y ~ T, data = data, distribution = asymptotic(), method = "holm", teststat = "quadratic")
  return(data.frame(statistic = NA, p.value = min(pair_tests$p.adjust)))
}


## dat2$TF <- factor(dat2$T)
## blah <- independence_test(Y~TF,data=dat2,xtrafo = mcp_trafo(TF = "Tukey")) #,teststat="quadratic")
## pair_tests2 <- coin::pvalue(blah,method="unadjusted")
## pair_tests2
## thecontrasts <- rbind("2 - 1 " = c(1,-1,0,0,0,0),
## "3-1"=c(1,0,-1,0,0,0))
## blah2<- independence_test(Y~TF,data=dat2,xtrafo = mcp_trafo(TF = thecontrasts))
## coin::pvalue(blah2,method="single-step")
##

ttest_T1_vs_all <- declare_test(handler = label_test(ttest_T1_vs_allfn), label = "t-test T1 vs. tous")
overall_T <- declare_test(handler = label_test(overall_Tfn), label = "test d'ensembe")
pairwise_test <- declare_test(handler = label_test(pairwise_testsfn), label = "Choix du meilleur test de paire")
pairwise_test_adj <- declare_test(handler = label_test(pairwise_tests_adjfn), label = "Choix du meilleur test de paire après ajustement")
```

```{r des2diag, cache=TRUE}
des2_plus <- des2 + ttest_T1_vs_all + overall_T + pairwise_test + pairwise_test_adj
thediagnosands <- declare_diagnosands(fwer = mean(p.value < .05))
des2_sim <- simulate_design(des2_plus, sims = 1000)
res2 <- des2_sim %>%
  group_by(estimator) %>%
  summarize(fwer = mean(p.value < .05), .groups = "drop")
kableExtra::kable(res2, caption = "Approaches to testing in multi-arm experiments.")
```

## Summary

- Multiple testing problems can arise from multiple outcomes or multiple  treatments (or multiple moderators/interaction terms).
  
- Procedures for making hypothesis tests and confidence intervals can involve  error. Ordinary practice controls the error rates in a single test (or single  confidence interval). But multiple tests require extra work to ensure that  error rates are controlled.

- The loss of power arising from adjustment approaches encourages us to   consider what *questions we want to ask of the data*. For example, if we   want to know if the treatment had *any effect*, then a joint test or omnibus   test of multiple outcomes will increase our statistical power without   requiring adjustment.

## Résumé

- Utiliser plusieurs résultats ou plusieurs traitements (ou plusieurs modérateurs/interactions) peut causer des problèmes de test.

- La procédure pour former les tests d'hypothèses et intervalles de confiance peut comporter des erreurs.
  Normalement on contrôle le taux d'erreur dans un seul test (ou un seul intervalle de confiance).
  Mais utliser plusieurs tests nécessite plus pour s'assurer que le taux d'erreur est sous contrôle.

- La perte de puissance statistique induite par les approches d'ajustement nous incite à réfléchir aux *questions que nous voulons poser sur les données*.
  Par exemple, si nous voulons savoir si le traitement a eu *un quelconque effet*, alors un test conjoint ou un test omnibus de résultats multiples augmentera notre puissance statistique sans nécessiter d'ajustement.

## Références {.allowframebreaks}
